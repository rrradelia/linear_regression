{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fe07af-6f61-49f3-a752-e320561c5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_symbol = sp.Symbol('x')\n",
    "\n",
    "class OneVariable: \n",
    "    def __init__(self, x, y):\n",
    "        self.x_mean = np.mean(x)\n",
    "        self.x_std = np.std(x)\n",
    "        self.y_mean = np.mean(y)\n",
    "        self.y_std = np.std(y)\n",
    "        self.x = (x - self.x_mean) / self.x_std\n",
    "        self.y = (y - self.y_mean) / self.y_std\n",
    "        self.m = 0 \n",
    "        self.b = 0\n",
    "        self.learningRate = 0.001\n",
    "        self.maxSteps = 1000\n",
    "        self.convergence_threshold = 1e-6\n",
    "        self.round = None\n",
    "        self.batch_size = 5\n",
    "\n",
    "    #Sum of Squared Residuals \n",
    "    def SoSR(self, m = None, b = None):\n",
    "        if m is None or b is None:\n",
    "            m = self.m\n",
    "            b = self.b\n",
    "        SoSR = np.square(self.y - m * self.x - b)\n",
    "        return np.sum(SoSR)\n",
    "\n",
    "    #Derivative of SoSR with respect to m varible\n",
    "    def dSoSR_dm(self, x_batch = None, y_batch = None):\n",
    "        if x_batch is None or y_batch is None:\n",
    "            x_batch = self.x\n",
    "            y_batch = self.y\n",
    "        dSoSR_dm = -2 * np.sum((y_batch - self.m * x_batch - self.b) * x_batch)\n",
    "        return dSoSR_dm\n",
    "\n",
    "    #Derivative of SoSR with respect to b varible\n",
    "    def dSoSR_db(self, x_batch = None, y_batch = None):\n",
    "        if x_batch is None or y_batch is None:\n",
    "            x_batch = self.x\n",
    "            y_batch = self.y\n",
    "        dSoSR_db = -2 * np.sum(y_batch - self.m * x_batch - self.b)\n",
    "        return dSoSR_db\n",
    "    \n",
    "    def LeastSquaresRegression(self):\n",
    "        print(\"\\033[1mLeast Squares Regression Line:\\033[0m\")\n",
    "        x_squares_sum = np.sum(np.square(self.x))\n",
    "        xy_sum = np.sum(self.x * self.y)\n",
    "        n = len(self.x)\n",
    "        self.m = (n * np.mean(self.x) * np.mean(self.y) - xy_sum) / (n * (np.mean(self.x) ** 2) - x_squares_sum)\n",
    "        self.b = (np.mean(self.y) - self.m * np.mean(self.x))\n",
    "        \n",
    "        self.show()\n",
    "\n",
    "    def GradientDescent(self):\n",
    "        print(\"\\033[1mGradient Descent:\\033[0m\")\n",
    "        losses = []\n",
    "        prev_loss = 0\n",
    "        for i in range(self.maxSteps):\n",
    "            m_gradient = self.dSoSR_dm()\n",
    "            b_gradient = self.dSoSR_db()\n",
    "            m_temp = self.m - self.learningRate * m_gradient\n",
    "            b_temp = self.b - self.learningRate * b_gradient\n",
    "            loss = self.SoSR(m_temp, b_temp)\n",
    "            losses.append(loss)\n",
    "\n",
    "            if abs(loss - prev_loss) < self.convergence_threshold:\n",
    "                print(\"Converged at iteration\", i)\n",
    "                break\n",
    "            prev_loss = loss\n",
    "            self.m = m_temp\n",
    "            self.b = b_temp\n",
    "\n",
    "        self.show(losses)\n",
    "\n",
    "    def MiniBatchGradientDescent(self):\n",
    "        print(\"\\033[1mMini Batch Gradient Descent:\\033[0m\")\n",
    "        losses = []\n",
    "        prev_loss = 0\n",
    "        for i in range(self.maxSteps):\n",
    "            for j in range(0, len(self.x), self.batch_size):\n",
    "                x_batch = self.x[j : j + self.batch_size]\n",
    "                y_batch = self.y[j : j + self.batch_size]\n",
    "                m_gradient = self.dSoSR_dm(x_batch, y_batch)\n",
    "                b_gradient = self.dSoSR_db(x_batch, y_batch)\n",
    "                m_temp = self.m - self.learningRate * m_gradient\n",
    "                b_temp = self.b - self.learningRate * b_gradient\n",
    "                loss = self.SoSR(m_temp, b_temp)\n",
    "                losses.append(loss)\n",
    "                self.m = m_temp\n",
    "                self.b = b_temp\n",
    "\n",
    "            if abs(loss - prev_loss) < self.convergence_threshold:\n",
    "                print(\"Converged at iteration\", i)\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "        self.show(losses)\n",
    "\n",
    "    def StochasticGradientDescent(self):\n",
    "        print(\"\\033[1mStochastic Gradient Descent:\\033[0m\")\n",
    "        losses = []\n",
    "        prev_loss = 0\n",
    "        for i in range(self.maxSteps):\n",
    "            for j in range(len(self.x)):\n",
    "                x_point = self.x[j]\n",
    "                y_point = self.y[j]\n",
    "                m_gradient = self.dSoSR_dm(np.array([x_point]), np.array([y_point]))\n",
    "                b_gradient = self.dSoSR_db(np.array([x_point]), np.array([y_point]))\n",
    "                m_temp = self.m - self.learningRate * m_gradient\n",
    "                b_temp = self.b - self.learningRate * b_gradient\n",
    "                loss = self.SoSR(m_temp, b_temp)\n",
    "                losses.append(loss)\n",
    "                self.m = m_temp\n",
    "                self.b = b_temp\n",
    "\n",
    "            if abs(loss - prev_loss) < self.convergence_threshold:\n",
    "                print(\"Converged at iteration\", i)\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "        self.show(losses)\n",
    "\n",
    "    def printEquation(self):\n",
    "        m = round(self.m * self.y_std / self.x_std, self.round) if self.round is not None else self.m * self.y_std / self.x_std\n",
    "        b = round(self.b * self.y_std + self.y_mean - m * self.x_mean * self.y_std / self.x_std, self.round) if self.round is not None else self.b * self.y_std + self.y_mean - m * self.x_mean * self.y_std / self.x_std\n",
    "        y = m * x_symbol + b\n",
    "        print(sp.pretty(y))\n",
    "\n",
    "\n",
    "    def plotResults(self):\n",
    "        plt.figure(figsize = (9, 7))\n",
    "        plt.scatter(self.x * self.x_std + self.x_mean, self.y * self.y_std + self.y_mean, color = 'blue', label = 'Data')\n",
    "        plt.plot(self.x * self.x_std + self.x_mean, (self.m * self.x + self.b) * self.y_std + self.y_mean, color = 'red', label = 'Regression Line')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Data and Regression Line')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plotLoss(self, losses):\n",
    "        plt.figure(figsize=(9, 7))\n",
    "        plt.plot(range(len(losses)), losses, label = 'Loss function')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def show(self, losses = None):\n",
    "        if losses != None:\n",
    "            self.printEquation()\n",
    "            self.plotResults()\n",
    "            self.plotLoss(losses)\n",
    "        else: \n",
    "            self.printEquation()\n",
    "            self.plotResults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
